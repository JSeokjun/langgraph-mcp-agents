{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP + LangGraph í•¸ì¦ˆì˜¨ íŠœí† ë¦¬ì–¼\n",
    "\n",
    "- ì‘ì„±ì: [í…Œë””ë…¸íŠ¸](https://youtube.com/c/teddynote)\n",
    "- ê°•ì˜: [íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤ RAG ë¹„ë²•ë…¸íŠ¸](https://fastcampus.co.kr/data_online_teddy)\n",
    "\n",
    "**ì°¸ê³ ìë£Œ**\n",
    "- https://modelcontextprotocol.io/introduction\n",
    "- https://github.com/langchain-ai/langchain-mcp-adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í™˜ê²½ì„¤ì •\n",
    "\n",
    "ì•„ë˜ ì„¤ì¹˜ ë°©ë²•ì„ ì°¸ê³ í•˜ì—¬ `uv` ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "**uv ì„¤ì¹˜ ë°©ë²•**\n",
    "\n",
    "```bash\n",
    "# macOS/Linux\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Windows (PowerShell)\n",
    "irm https://astral.sh/uv/install.ps1 | iex\n",
    "```\n",
    "\n",
    "**ì˜ì¡´ì„± ì„¤ì¹˜**\n",
    "\n",
    "```bash\n",
    "uv pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í™˜ê²½ë³€ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "[í”„ë¡œì íŠ¸ëª…]\n",
      "LangGraph MCP Adapters HandsOn\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "logging.langsmith(\"LangGraph MCP Adapters HandsOn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiServerMCPClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‚¬ì „ì— `mcp_server_remote.py` ë¥¼ ì‹¤í–‰í•´ë‘¡ë‹ˆë‹¤. í„°ë¯¸ë„ì„ ì—´ê³  ê°€ìƒí™˜ê²½ì´ í™œì„±í™” ë˜ì–´ ìˆëŠ” ìƒíƒœì—ì„œ ì„œë²„ë¥¼ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "> ëª…ë ¹ì–´\n",
    "```bash\n",
    "source .venv/bin/activate\n",
    "python mcp_server_remote.py\n",
    "```\n",
    "\n",
    "`async with` ë¡œ ì¼ì‹œì ì¸ Session ì—°ê²°ì„ ìƒì„± í›„ í•´ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='get_weather', description='\\n    Get current weather information for the specified location.\\n\\n    This function simulates a weather service by returning a fixed response.\\n    In a production environment, this would connect to a real weather API.\\n\\n    Args:\\n        location (str): The name of the location (city, region, etc.) to get weather for\\n\\n    Returns:\\n        str: A string containing the weather information for the specified location\\n    ', args_schema={'properties': {'location': {'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'title': 'get_weatherArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x7201ac63d260>)]\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mtools\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "It's always Sunny in ì„œìš¸\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì„œìš¸ì€ í•­ìƒ ë§‘ìŠµë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_teddynote.messages import ainvoke_graph, astream_graph\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-lite\", \n",
    "    temperature=0,\n",
    "    max_tokens=20000,\n",
    ")\n",
    "\n",
    "async with MultiServerMCPClient(\n",
    "    {\n",
    "        \"weather\": {\n",
    "            # ì„œë²„ì˜ í¬íŠ¸ì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.(8005ë²ˆ í¬íŠ¸)\n",
    "            \"url\": \"http://localhost:8005/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ") as client:\n",
    "    print(client.get_tools())\n",
    "    agent = create_react_agent(model, client.get_tools())\n",
    "    answer = await astream_graph(agent, {\"messages\": \"ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ì–´ë– ë‹ˆ?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒì˜ ê²½ìš°ì—ëŠ” session ì´ ë‹«í˜”ê¸° ë•Œë¬¸ì— ë„êµ¬ì— ì ‘ê·¼í•  ìˆ˜ ì—†ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mtools\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Error: ClosedResourceError()\n",
      " Please fix your mistakes.\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì£„ì†¡í•©ë‹ˆë‹¤. ì„œìš¸ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node': 'agent',\n",
       " 'content': AIMessageChunk(content='ë„í•´ ì£¼ì„¸ìš”.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run-e390f857-22b9-411b-b358-91176b89c355', usage_metadata={'input_tokens': -50, 'output_tokens': 27, 'total_tokens': -23, 'input_token_details': {'cache_read': 0}}),\n",
       " 'metadata': {'langgraph_step': 3,\n",
       "  'langgraph_node': 'agent',\n",
       "  'langgraph_triggers': ('branch:to:agent', 'start:agent', 'tools'),\n",
       "  'langgraph_path': ('__pregel_pull', 'agent'),\n",
       "  'langgraph_checkpoint_ns': 'agent:8f285b9b-b389-3753-5c74-59e317a89244',\n",
       "  'checkpoint_ns': 'agent:8f285b9b-b389-3753-5c74-59e317a89244',\n",
       "  'ls_provider': 'google_genai',\n",
       "  'ls_model_name': 'models/gemini-2.0-flash-lite',\n",
       "  'ls_model_type': 'chat',\n",
       "  'ls_temperature': 0.0,\n",
       "  'ls_max_tokens': 20000}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await astream_graph(agent, {\"messages\": \"ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ì–´ë– ë‹ˆ?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ê·¸ëŸ¼ Async Session ì„ ìœ ì§€í•˜ë©° ë„êµ¬ì— ì ‘ê·¼í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½í•´ ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='get_weather', description='\\n    Get current weather information for the specified location.\\n\\n    This function simulates a weather service by returning a fixed response.\\n    In a production environment, this would connect to a real weather API.\\n\\n    Args:\\n        location (str): The name of the location (city, region, etc.) to get weather for\\n\\n    Returns:\\n        str: A string containing the weather information for the specified location\\n    ', args_schema={'properties': {'location': {'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'title': 'get_weatherArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x7201ac2839c0>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in sse_reader: peer closed connection without sending complete message body (incomplete chunked read)\n"
     ]
    }
   ],
   "source": [
    "# 1. í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"weather\": {\n",
    "            \"url\": \"http://localhost:8005/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# 2. ëª…ì‹œì ìœ¼ë¡œ ì—°ê²° ì´ˆê¸°í™” (ì´ ë¶€ë¶„ì´ í•„ìš”í•¨)\n",
    "# ì´ˆê¸°í™”\n",
    "await client.__aenter__()\n",
    "\n",
    "# ì´ì œ ë„êµ¬ê°€ ë¡œë“œë¨\n",
    "print(client.get_tools())  # ë„êµ¬ê°€ í‘œì‹œë¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langgraph ì˜ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—ì´ì „íŠ¸ ìƒì„±\n",
    "agent = create_react_agent(model, client.get_tools())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mtools\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "It's always Sunny in ì„œìš¸\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì„œìš¸ì€ í•­ìƒ ë§‘ìŠµë‹ˆë‹¤."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node': 'agent',\n",
       " 'content': AIMessageChunk(content='ì€ í•­ìƒ ë§‘ìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run-11494a9a-c44e-46c9-a4bc-b451f182715d', usage_metadata={'input_tokens': -51, 'output_tokens': 9, 'total_tokens': -42, 'input_token_details': {'cache_read': 0}}),\n",
       " 'metadata': {'langgraph_step': 3,\n",
       "  'langgraph_node': 'agent',\n",
       "  'langgraph_triggers': ('branch:to:agent', 'start:agent', 'tools'),\n",
       "  'langgraph_path': ('__pregel_pull', 'agent'),\n",
       "  'langgraph_checkpoint_ns': 'agent:a79d5197-d417-7abc-25e0-1f680eec8156',\n",
       "  'checkpoint_ns': 'agent:a79d5197-d417-7abc-25e0-1f680eec8156',\n",
       "  'ls_provider': 'google_genai',\n",
       "  'ls_model_name': 'models/gemini-2.0-flash-lite',\n",
       "  'ls_model_type': 'chat',\n",
       "  'ls_temperature': 0.0,\n",
       "  'ls_max_tokens': 20000}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await astream_graph(agent, {\"messages\": \"ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ì–´ë– ë‹ˆ?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stdio í†µì‹  ë°©ì‹\n",
    "\n",
    "Stdio í†µì‹  ë°©ì‹ì€ ë¡œì»¬ í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "- í†µì‹ ì„ ìœ„í•´ í‘œì¤€ ì…ë ¥/ì¶œë ¥ ì‚¬ìš©\n",
    "\n",
    "ì°¸ê³ : ì•„ë˜ì˜ python ê²½ë¡œëŠ” ìˆ˜ì •í•˜ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='get_weather', description='\\n    Get current weather information for the specified location.\\n\\n    This function simulates a weather service by returning a fixed response.\\n    In a production environment, this would connect to a real weather API.\\n\\n    Args:\\n        location (str): The name of the location (city, region, etc.) to get weather for\\n\\n    Returns:\\n        str: A string containing the weather information for the specified location\\n    ', args_schema={'properties': {'location': {'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'title': 'get_weatherArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x72018830e7a0>)]\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mtools\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "It's always Sunny in ì„œìš¸\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì„œìš¸ì€ í•­ìƒ ë§‘ìŠµë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Googleì˜ Gemini ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-lite\", \n",
    "    temperature=0,\n",
    "    max_tokens=20000,\n",
    ")\n",
    "\n",
    "# StdIO ì„œë²„ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "# - command: Python ì¸í„°í”„ë¦¬í„° ê²½ë¡œ\n",
    "# - args: ì‹¤í–‰í•  MCP ì„œë²„ ìŠ¤í¬ë¦½íŠ¸\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"./mcp_test/bin/python\",\n",
    "    args=[\"mcp_server_local.py\"],\n",
    ")\n",
    "\n",
    "# StdIO í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œë²„ì™€ í†µì‹ \n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    # í´ë¼ì´ì–¸íŠ¸ ì„¸ì…˜ ìƒì„±\n",
    "    async with ClientSession(read, write) as session:\n",
    "        # ì—°ê²° ì´ˆê¸°í™”\n",
    "        await session.initialize()\n",
    "\n",
    "        # MCP ë„êµ¬ ë¡œë“œ\n",
    "        tools = await load_mcp_tools(session)\n",
    "        print(tools)\n",
    "\n",
    "        # ì—ì´ì „íŠ¸ ìƒì„±\n",
    "        agent = create_react_agent(model, tools)\n",
    "\n",
    "        # ì—ì´ì „íŠ¸ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°\n",
    "        await astream_graph(agent, {\"messages\": \"ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ì–´ë– ë‹ˆ?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG ë¥¼ êµ¬ì¶•í•œ MCP ì„œë²„ ì‚¬ìš©\n",
    "\n",
    "- íŒŒì¼: `mcp_server_rag.py`\n",
    "\n",
    "ì‚¬ì „ì— langchain ìœ¼ë¡œ êµ¬ì¶•í•œ `mcp_server_rag.py` íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "stdio í†µì‹  ë°©ì‹ìœ¼ë¡œ ë„êµ¬ì— ëŒ€í•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ì—¬ê¸°ì„œ ë„êµ¬ëŠ” `retriever` ë„êµ¬ë¥¼ ê°€ì ¸ì˜¤ê²Œ ë˜ë©°, ì´ ë„êµ¬ëŠ” `mcp_server_rag.py` ì—ì„œ ì •ì˜ëœ ë„êµ¬ì…ë‹ˆë‹¤. ì´ íŒŒì¼ì€ ì‚¬ì „ì— ì„œë²„ì—ì„œ ì‹¤í–‰ë˜ì§€ **ì•Šì•„ë„** ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_teddynote.messages import astream_graph\n",
    "\n",
    "# Googleì˜ Gemini ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\", \n",
    "    temperature=0,\n",
    "    max_tokens=20000,\n",
    ")\n",
    "\n",
    "# RAG ì„œë²„ë¥¼ ìœ„í•œ StdIO ì„œë²„ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"./mcp_test/bin/python\",\n",
    "    args=[\"./mcp_server_rag.py\"],\n",
    ")\n",
    "\n",
    "# StdIO í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ì„œë²„ì™€ í†µì‹ \n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    \n",
    "    # í´ë¼ì´ì–¸íŠ¸ ì„¸ì…˜ ìƒì„±\n",
    "    async with ClientSession(read, write) as session:\n",
    "        # ì—°ê²° ì´ˆê¸°í™”\n",
    "        await session.initialize()\n",
    "\n",
    "        # MCP ë„êµ¬ ë¡œë“œ (ì—¬ê¸°ì„œëŠ” retriever ë„êµ¬)\n",
    "        tools = await load_mcp_tools(session)\n",
    "\n",
    "        # ì—ì´ì „íŠ¸ ìƒì„± ë° ì‹¤í–‰\n",
    "        agent = create_react_agent(model, tools)\n",
    "\n",
    "        # ì—ì´ì „íŠ¸ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°\n",
    "        await astream_graph(\n",
    "            agent, {\"messages\": \"PDF íŒŒì¼ì— ì–´ë–¤ ë‚´ìš©ì´ ìˆë‚˜ìš”?\"}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSE ë°©ì‹ê³¼ StdIO ë°©ì‹ í˜¼í•© ì‚¬ìš©\n",
    "\n",
    "- íŒŒì¼: `mcp_server_rag.py` ëŠ” StdIO ë°©ì‹ìœ¼ë¡œ í†µì‹ \n",
    "- `langchain-dev-docs` ëŠ” SSE ë°©ì‹ìœ¼ë¡œ í†µì‹ \n",
    "\n",
    "SSE ë°©ì‹ê³¼ StdIO ë°©ì‹ì„ í˜¼í•©í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Anthropicì˜ Claude ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatAnthropic(\n",
    "    model_name=\"claude-3-7-sonnet-latest\", temperature=0, max_tokens=20000\n",
    ")\n",
    "\n",
    "# 1. ë‹¤ì¤‘ ì„œë²„ MCP í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"document-retriever\": {\n",
    "            \"command\": \"./.venv/bin/python\",\n",
    "            # mcp_server_rag.py íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤\n",
    "            \"args\": [\"./mcp_server_rag.py\"],\n",
    "            # stdio ë°©ì‹ìœ¼ë¡œ í†µì‹  (í‘œì¤€ ì…ì¶œë ¥ ì‚¬ìš©)\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"langchain-dev-docs\": {\n",
    "            # SSE ì„œë²„ê°€ 8765 í¬íŠ¸ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”\n",
    "            \"url\": \"http://teddynote.io:8765/sse\",\n",
    "            # SSE(Server-Sent Events) ë°©ì‹ìœ¼ë¡œ í†µì‹ \n",
    "            \"transport\": \"sse\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# 2. ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¥¼ í†µí•œ ëª…ì‹œì  ì—°ê²° ì´ˆê¸°í™”\n",
    "await client.__aenter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langgraph ì˜ `create_react_agent` ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "prompt = (\n",
    "    \"You are a smart agent. \"\n",
    "    \"Use `retriever` tool to search on AI related documents and answer questions.\"\n",
    "    \"Use `langchain-dev-docs` tool to search on langchain / langgraph related documents and answer questions.\"\n",
    "    \"Answer in Korean.\"\n",
    ")\n",
    "agent = create_react_agent(\n",
    "    model, client.get_tools(), prompt=prompt, checkpointer=MemorySaver()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "êµ¬ì¶•í•´ ë†“ì€ `mcp_server_rag.py` ì—ì„œ ì •ì˜í•œ `retriever` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RunnableConfig(recursion_limit=30, thread_id=1)\n",
    "await astream_graph(\n",
    "    agent,\n",
    "    {\n",
    "        \"messages\": \"`retriever` ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AI ì´ë¦„ì„ ê²€ìƒ‰í•´ì¤˜\"\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë²ˆì—ëŠ” `langchain-dev-docs` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RunnableConfig(recursion_limit=30, thread_id=1)\n",
    "await astream_graph(\n",
    "    agent,\n",
    "    {\"messages\": \"langgraph-dev-docs ì°¸ê³ í•´ì„œ self-rag ì˜ ì •ì˜ì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MemorySaver` ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ê¸° ê¸°ì–µì„ ìœ ì§€í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, multi-turn ëŒ€í™”ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(\n",
    "    agent, {\"messages\": \"ì´ì „ì˜ ë‚´ìš©ì„ bullet point ë¡œ ìš”ì•½í•´ì¤˜\"}, config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain ì— í†µí•©ëœ ë„êµ¬ + MCP ë„êµ¬\n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” LangChain ì— í†µí•©ëœ ë„êµ¬ë¥¼ ê¸°ì¡´ì˜ MCP ë¡œë§Œ ì´ë£¨ì–´ì§„ ë„êµ¬ì™€ í•¨ê»˜ ì‚¬ìš©ì´ ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "\n",
    "# Tavily ê²€ìƒ‰ ë„êµ¬ë¥¼ ì´ˆê¸°í™” í•©ë‹ˆë‹¤. (news íƒ€ì…, ìµœê·¼ 3ì¼ ë‚´ ë‰´ìŠ¤)\n",
    "tavily = TavilySearch(max_results=3, topic=\"news\", days=3)\n",
    "\n",
    "# ê¸°ì¡´ì˜ MCP ë„êµ¬ì™€ í•¨ê»˜ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "tools = client.get_tools() + [tavily]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langgraph ì˜ `create_react_agent` ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "prompt = \"You are a smart agent with various tools. Answer questions in Korean.\"\n",
    "agent = create_react_agent(model, tools, prompt=prompt, checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìƒˆë¡­ê²Œ ì¶”ê°€í•œ `tavily` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(agent, {\"messages\": \"ì˜¤ëŠ˜ ë‰´ìŠ¤ ì°¾ì•„ì¤˜\"}, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`retriever` ë„êµ¬ê°€ ì›í™œí•˜ê²Œ ì‘ë™í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(\n",
    "    agent,\n",
    "    {\n",
    "        \"messages\": \"`retriever` ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AI ì´ë¦„ì„ ê²€ìƒ‰í•´ì¤˜\"\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smithery ì—ì„œ ì œê³µí•˜ëŠ” MCP ì„œë²„\n",
    "\n",
    "- ë§í¬: https://smithery.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‚¬ìš©í•œ ë„êµ¬ ëª©ë¡ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "- Sequential Thinking: https://smithery.ai/server/@smithery-ai/server-sequential-thinking\n",
    "  - êµ¬ì¡°í™”ëœ ì‚¬ê³  í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ì—­ë™ì ì´ê³  ì„±ì°°ì ì¸ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ë„êµ¬ë¥¼ ì œê³µí•˜ëŠ” MCP ì„œë²„\n",
    "- Desktop Commander: https://smithery.ai/server/@wonderwhy-er/desktop-commander\n",
    "  - ë‹¤ì–‘í•œ í¸ì§‘ ê¸°ëŠ¥ìœ¼ë¡œ í„°ë¯¸ë„ ëª…ë ¹ì„ ì‹¤í–‰í•˜ê³  íŒŒì¼ì„ ê´€ë¦¬í•˜ì„¸ìš”. ì½”ë”©, ì…¸ ë° í„°ë¯¸ë„, ì‘ì—… ìë™í™”\n",
    "\n",
    "**ì°¸ê³ **\n",
    "\n",
    "- smithery ì—ì„œ ì œê³µí•˜ëŠ” ë„êµ¬ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ê°€ì ¸ì˜¬ë•Œ, ì•„ë˜ì˜ ì˜ˆì‹œì²˜ëŸ¼ `\"transport\": \"stdio\"` ë¡œ ê¼­ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_mcp_adapters.client.MultiServerMCPClient at 0x720175dadfa0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# LLM ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "# 1. í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        # \"server-sequential-thinking\": {\n",
    "        #     \"command\": \"npx\",\n",
    "        #     \"args\": [\n",
    "        #         \"-y\",\n",
    "        #         \"@smithery/cli@latest\",\n",
    "        #         \"run\",\n",
    "        #         \"@smithery-ai/server-sequential-thinking\",\n",
    "        #         \"--key\",\n",
    "        #         \"a94d0233-912b-4752-82e4-78949f447de7\"\n",
    "        #     ],\n",
    "        #     \"transport\": \"stdio\",  # stdio ë°©ì‹ìœ¼ë¡œ í†µì‹ ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "        # },\n",
    "        \"desktop-commander\": {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"-y\",\n",
    "                \"@smithery/cli@latest\",\n",
    "                \"run\",\n",
    "                \"@wonderwhy-er/desktop-commander\",\n",
    "                \"--key\",\n",
    "                \"a94d0233-912b-4752-82e4-78949f447de7\"\n",
    "            ],\n",
    "            \"transport\": \"stdio\",  # stdio ë°©ì‹ìœ¼ë¡œ í†µì‹ ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "        },\n",
    "        # \"document-retriever\": {\n",
    "        #     \"command\": \"./.venv/bin/python\",\n",
    "        #     # mcp_server_rag.py íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤\n",
    "        #     \"args\": [\"./mcp_server_rag.py\"],\n",
    "        #     # stdio ë°©ì‹ìœ¼ë¡œ í†µì‹  (í‘œì¤€ ì…ì¶œë ¥ ì‚¬ìš©)\n",
    "        #     \"transport\": \"stdio\",\n",
    "        # },\n",
    "        \"duckduckgo-mcp-server\": {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"-y\",\n",
    "                \"@smithery/cli@latest\",\n",
    "                \"run\",\n",
    "                \"@nickclyde/duckduckgo-mcp-server\",\n",
    "                \"--key\",\n",
    "                \"a94d0233-912b-4752-82e4-78949f447de7\"\n",
    "            ],\n",
    "            \"transport\": \"stdio\",  # stdio ë°©ì‹ìœ¼ë¡œ í†µì‹ ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# 2. ëª…ì‹œì ìœ¼ë¡œ ì—°ê²° ì´ˆê¸°í™”\n",
    "await client.__aenter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langgraph ì˜ `create_react_agent` ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(recursion_limit=30, thread_id=2)\n",
    "\n",
    "agent = create_react_agent(model, client.get_tools(), checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Desktop Commander` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í„°ë¯¸ë„ ëª…ë ¹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key '$schema' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key '$schema' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key '$schema' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key '$schema' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key '$schema' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key '$schema' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key '$schema' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key '$schema' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key '$schema' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key '$schema' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key '$schema' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key '$schema' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key '$schema' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key '$schema' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key '$schema' is not supported in schema, ignoring\n",
      "Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key '$schema' is not supported in schema, ignoring\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "í˜„ì¬ ê²½ë¡œë¥¼ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, tree êµ¬ì¡°ë¥¼ ê·¸ë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ê²½ë¡œë¥¼ ì•Œë ¤ì£¼ì‹œê±°ë‚˜, íŠ¹ì • ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì‹œë©´ í•´ë‹¹ ê²½ë¡œë¥¼ ê¸°ì¤€ìœ¼ë¡œ tree êµ¬ì¡°ë¥¼ ê·¸ë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë˜í•œ, mcp_test í´ë”ë¥¼ ì œì™¸í•˜ëŠ” ì˜µì…˜ë„ ì ìš©í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node': 'agent',\n",
       " 'content': AIMessageChunk(content=' ê·¸ë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë˜í•œ, mcp_test í´ë”ë¥¼ ì œì™¸í•˜ëŠ” ì˜µì…˜ë„ ì ìš©í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-5f19a43c-d037-4be4-8954-c7e29471e1aa', usage_metadata={'input_tokens': -700, 'output_tokens': 86, 'total_tokens': -614, 'input_token_details': {'cache_read': 0}}),\n",
       " 'metadata': {'thread_id': 2,\n",
       "  'langgraph_step': 1,\n",
       "  'langgraph_node': 'agent',\n",
       "  'langgraph_triggers': ('branch:to:agent', 'start:agent', 'tools'),\n",
       "  'langgraph_path': ('__pregel_pull', 'agent'),\n",
       "  'langgraph_checkpoint_ns': 'agent:883842a7-c389-3fd8-4f37-31172825ba4c',\n",
       "  'checkpoint_ns': 'agent:883842a7-c389-3fd8-4f37-31172825ba4c',\n",
       "  'ls_provider': 'google_genai',\n",
       "  'ls_model_name': 'models/gemini-2.0-flash',\n",
       "  'ls_model_type': 'chat',\n",
       "  'ls_temperature': 0.7}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await astream_graph(\n",
    "    agent,\n",
    "    {\n",
    "        \"messages\": \"í˜„ì¬ ê²½ë¡œë¥¼ í¬í•¨í•œ í•˜ìœ„ í´ë” êµ¬ì¡°ë¥¼ tree ë¡œ ê·¸ë ¤ì¤˜. ë‹¨, mcp_test í´ë”ëŠ” ì œì™¸í•˜ê³  ì¶œë ¥í•´ì¤˜.\"\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë²ˆì—ëŠ” `Sequential Thinking` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„êµì  ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m astream_graph(\n\u001b[32m      2\u001b[39m     agent,\n\u001b[32m      3\u001b[39m     {\n\u001b[32m      4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: (\n\u001b[32m      5\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`DuckDuckGo Search Server` ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ì†í¥ë¯¼ì— ëŒ€í•œ ìµœì‹  ê¸°ì‚¬ 1ê°œë¥¼ ê²€ìƒ‰í•´ì„œ ì¶œë ¥í•´ì¤˜.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m         )\n\u001b[32m      7\u001b[39m     },\n\u001b[32m      8\u001b[39m     config=config,\n\u001b[32m      9\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/mcp_test/langgraph-mcp-agents/mcp_test/lib/python3.12/site-packages/langchain_teddynote/messages.py:488\u001b[39m, in \u001b[36mastream_graph\u001b[39m\u001b[34m(graph, inputs, config, node_names, callback, stream_mode, include_subgraphs)\u001b[39m\n\u001b[32m    485\u001b[39m prev_node = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk_msg, metadata \u001b[38;5;129;01min\u001b[39;00m graph.astream(\n\u001b[32m    489\u001b[39m         inputs, config, stream_mode=stream_mode\n\u001b[32m    490\u001b[39m     ):\n\u001b[32m    491\u001b[39m         curr_node = metadata[\u001b[33m\"\u001b[39m\u001b[33mlanggraph_node\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    492\u001b[39m         final_result = {\u001b[33m\"\u001b[39m\u001b[33mnode\u001b[39m\u001b[33m\"\u001b[39m: curr_node, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: chunk_msg, \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/mcp_test/langgraph-mcp-agents/mcp_test/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2621\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[39m\n\u001b[32m   2615\u001b[39m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2616\u001b[39m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[32m   2617\u001b[39m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2618\u001b[39m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2619\u001b[39m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[32m   2620\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2621\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2622\u001b[39m         loop.tasks.values(),\n\u001b[32m   2623\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2624\u001b[39m         retry_policy=\u001b[38;5;28mself\u001b[39m.retry_policy,\n\u001b[32m   2625\u001b[39m         get_waiter=get_waiter,\n\u001b[32m   2626\u001b[39m     ):\n\u001b[32m   2627\u001b[39m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2628\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[32m   2629\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project/mcp_test/langgraph-mcp-agents/mcp_test/lib/python3.12/site-packages/langgraph/pregel/runner.py:328\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    326\u001b[39m end_time = timeout + loop.time() \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(futures) > (\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_waiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     done, inflight = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.wait(\n\u001b[32m    329\u001b[39m         futures,\n\u001b[32m    330\u001b[39m         return_when=asyncio.FIRST_COMPLETED,\n\u001b[32m    331\u001b[39m         timeout=(\u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, end_time - loop.time()) \u001b[38;5;28;01mif\u001b[39;00m end_time \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m    334\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# timed out\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/tasks.py:464\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(fs, timeout, return_when)\u001b[39m\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPassing coroutines is forbidden, use tasks explicitly.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    463\u001b[39m loop = events.get_running_loop()\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _wait(fs, timeout, return_when, loop)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/tasks.py:550\u001b[39m, in \u001b[36m_wait\u001b[39m\u001b[34m(fs, timeout, return_when, loop)\u001b[39m\n\u001b[32m    547\u001b[39m     f.add_done_callback(_on_completion)\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    552\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "await astream_graph(\n",
    "    agent,\n",
    "    {\n",
    "        \"messages\": (\n",
    "            \"`DuckDuckGo Search Server` ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ì†í¥ë¯¼ì— ëŒ€í•œ ìµœì‹  ê¸°ì‚¬ 1ê°œë¥¼ ê²€ìƒ‰í•´ì„œ ì¶œë ¥í•´ì¤˜.\"\n",
    "        )\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
